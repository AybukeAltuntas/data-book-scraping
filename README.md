# Scraping Egzersizi â€” Python ile Web Sitesinden Kitap Verisi Ã‡ekme

Bu alÄ±ÅŸtÄ±rmada, derste ele alÄ±nan Scraping tekniklerini pratiÄŸe dÃ¶keceÄŸiz. AmacÄ±mÄ±z, Python kullanarak bir web sitesinden otomatik olarak bilgi Ã§Ä±karmak olacaktÄ±r.

Scraping yapacaÄŸÄ±mÄ±z web sitesi: [books.toscrape.com](http://books.toscrape.com/). Bu site tam olarak bizim amacÄ±mÄ±z iÃ§in oluÅŸturulmuÅŸtur â€” scraping nasÄ±l yapÄ±lÄ±r Ã¶ÄŸrenmek iÃ§in!

Hedefimiz, satÄ±lan kitaplar hakkÄ±nda isimleri, fiyatlarÄ±, rating deÄŸerleri vb. bilgileri otomatik olarak elde etmektir. Ä°ÅŸin pÃ¼f noktasÄ± ise web sitesinin **paginated** olmasÄ±dÄ±r. Bunu fark edebiliyor musun? Bunun bir zorluk yaratacaÄŸÄ±nÄ± Ã¶ngÃ¶rÃ¼yor musun?

## Setup

AmaÃ§, web sitesini scrape etmek **ve** ardÄ±ndan elde edilen bilgileri `pandas` kullanarak gÃ¶rselleÅŸtirmektir. Bu alÄ±ÅŸtÄ±rma iÃ§in Notebook iÃ§inde Ã§alÄ±ÅŸmak hÃ¢lÃ¢ mantÄ±klÄ±dÄ±r.

```bash
jupyter notebook
```

`~/code/<user.github_nickname>/{{local_path_to("02-Data-Toolkit/02-Data-Sourcing/02-Scraping")}}` klasÃ¶rÃ¼ndeki yeni Python Notebook dosyasÄ±nÄ± aÃ§.

Python dÃ¼nyasÄ±nda scraping iÃ§in kullanÄ±mÄ± kolay bir library olan `BeautifulSoup` mevcuttur. Kurulum sÄ±rasÄ±nda bunu yÃ¼klemiÅŸtik, dolayÄ±sÄ±yla doÄŸrudan import edebiliriz.

Bunu `requests` libraryâ€™si ile birlikte kullanacaÄŸÄ±z: `requests` HTML sayfasÄ±nÄ± Ã§ekmemize yardÄ±mcÄ± olacak, `BeautifulSoup` ise bu sayfadan bilgileri extract etmemizi saÄŸlayacaktÄ±r.

Notebookâ€™un ilk code cellâ€™ine aÅŸaÄŸÄ±daki importâ€™larÄ± ekleyerek baÅŸla:

```python
import requests
from bs4 import BeautifulSoup

import numpy as np
import pandas as pd

%matplotlib inline
import matplotlib.pyplot as plt
```

## First request

Yeni bir cell ekle ve aÅŸaÄŸÄ±daki `TODO` alanlarÄ± Ã¼zerinde Ã§alÄ±ÅŸ (baÅŸlangÄ±Ã§ kodu derste kullanÄ±lan slaytlardakiyle aynÄ±dÄ±r!):

```python
url = "http://books.toscrape.com/"

# TODO: Use `requests` to do an HTTP request to fetch data located at that URL
# TODO: Create a `BeautifulSoup` instance with that data
```

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le
</summary>

Bu kod oldukÃ§a geneldir ve derste gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zle aynÄ± olmalÄ±dÄ±r! EÄŸer zaten bir scraping projen varsa, genellikle yaptÄ±ÄŸÄ±n ÅŸey projeyi aÃ§Ä±p bu ilk satÄ±rlarÄ± kopyalayÄ±p yapÄ±ÅŸtÄ±rmaktÄ±r!

```python
url = "http://books.toscrape.com/"

# This is where we do an HTTP request to get the HTML from the website
response = requests.get(url)

# And this is where we feed that HTML to the parser
soup = BeautifulSoup(response.content, "html.parser")
```

</details>

Yeni bir code cell iÃ§inde `soup` deÄŸiÅŸkenini incele. Tipi nedir? Ä°lk bakÄ±ÅŸta string gibi gÃ¶rÃ¼nebilir, fakat gerÃ§ekten Ã¶yle mi? `type(soup)` ile kontrol et.

`soup` artÄ±k HTML Ã¼zerinde sorgular Ã§alÄ±ÅŸtÄ±rabileceÄŸimiz parser iÃ§eren bir BeautifulSoup objectâ€™idir. Hangi HTML elementlerini extract etmek istediÄŸini belirlemek iÃ§in, *Books to Scrape* web sitesinin HTML yapÄ±sÄ±nÄ± browser inspector ile analiz etmen gerekir.

Browser inspector kullanmak iÃ§in, incelemek istediÄŸin elemente saÄŸ tÄ±kla ve menÃ¼den `Inspect` seÃ§eneÄŸini belirle.

![Website ve inspector ekran gÃ¶rÃ¼ntÃ¼sÃ¼](img dosyasÄ±nda)

Tek bir kitabÄ± iÃ§eren HTML yapÄ±sÄ±nÄ± fark edebiliyor musun? Her kitap iÃ§in bu yapÄ± aynÄ± mÄ±?

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le
</summary>

AradÄ±ÄŸÄ±mÄ±z yapÄ±, `product_pod` classâ€™Ä±na sahip `<article />` elementidir! Sayfadaki tÃ¼m kitaplar birebir aynÄ± structureâ€™a sahiptir, parsing iÃ§in tam olarak ihtiyacÄ±mÄ±z olan da budur.

```html
<article class="product_pod">
  <!-- [...] -->
</article>
```

</details>

ArtÄ±k ilgili HTMLâ€™yi tespit ettiÄŸimize gÃ¶re, `soup` Python deÄŸiÅŸkenini kullanarak document Ã¼zerinde sorgu yapabiliriz. [searching by CSS class](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-by-css-class) yaklaÅŸÄ±mÄ±nÄ± kullanalÄ±m. Yeni bir cell ekleyerek HTML iÃ§indeki **tÃ¼m** kitaplarÄ± seÃ§meyi dene ve bunu `books_html` deÄŸiÅŸkenine ata.

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le
</summary>

```python
books_html = soup.find_all("article", class_="product_pod")
len(books_html)
```

</details>

ArtÄ±k tÃ¼m `<article />` elementlerini iÃ§eren bir `books_html` deÄŸiÅŸkenimiz var, ÅŸimdi **tek bir** kitap (ilk kitap!) Ã¼zerinde odaklanalÄ±m ve bu HTML parÃ§asÄ±ndan ihtiyacÄ±mÄ±z olan tÃ¼m bilgileri extract etmeye Ã§alÄ±ÅŸalÄ±m.

## Parsing *one* book

Bu noktada bir **Markdown cell** ekleyip ÅŸunu yazman iyi olacaktÄ±r:

```markdown
## Parsing _one_ book
```

Elbette daha fazla metin de yazabilirsin! Buradaki amaÃ§, Notebook iÃ§inde dÃ¼ÅŸÃ¼nce sÃ¼recini dokÃ¼mante ederek iyi yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir Notebook elde etmektir.

Ä°lk kitabÄ±n HTML parÃ§asÄ±na bakalÄ±m. Yeni bir code cell ekle ve ÅŸunu yaz:

```python
books_html[0]
```

Harika! ArtÄ±k uÄŸraÅŸacaÄŸÄ±mÄ±z daha kÃ¼Ã§Ã¼k bir HTML parÃ§amÄ±z var.

String gibi gÃ¶rÃ¼nÃ¼yor, deÄŸil mi? Ama artÄ±k daha iyisini biliyoruz! `type()` ile kontrol et. Tekrar daha geliÅŸmiÅŸ bir object olduÄŸunu gÃ¶receksin.

Bu HTML parÃ§asÄ± Ã¼zerinde [`.find()`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find) methodâ€™unu zincirleme kullanarak 3 farklÄ± bilgiyi extract edebiliriz.

### Kitap baÅŸlÄ±ÄŸÄ±nÄ± bulmak â€” attribute iÃ§inden text almak

Kitap *title*â€™Ä± ile baÅŸlayalÄ±m. Bu bilgiyi `books_html[0]` iÃ§inden elde etmeye Ã§alÄ±ÅŸ ve `book_title` deÄŸiÅŸkenine ata.

BaÅŸlÄ±ÄŸÄ± Notebookâ€™ta gÃ¶sterdiÄŸin HTML kodu iÃ§inde bulmayÄ± deneyebilirsin. Alternatif olarak browserâ€™a dÃ¶nÃ¼p tekrar Inspect fonksiyonunu kullanarak baÅŸlÄ±ÄŸÄ± iÃ§eren elementi bulabilir ve oradan `<article>` seviyesine kadar yukarÄ± Ã§Ä±kabilirsin.

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le (sadece <strong>kendin denedikten sonra</strong>!)
</summary>

BaÅŸlÄ±k `<h3 />` tagâ€™inin iÃ§indeki `<a />` link tagâ€™inde yer alÄ±r. Ã–nce `h3`, ardÄ±ndan `a` elementini bulmamÄ±z gerekir:

```python
books_html[0].find("h3").find("a")
```

Neredeyse tamam. Åimdi `<a />` tagâ€™inin **attributes** alanÄ±ndaki title deÄŸerini seÃ§meliyiz:

```python
books_html[0].find("h3").find("a").attrs
```

Bu satÄ±r bir `dict` dÃ¶ndÃ¼rÃ¼r. ArtÄ±k doÄŸru keyâ€™i seÃ§ebilirsin:

```python
book_title = books_html[0].find("h3").find("a").attrs["title"]
book_title
```

</details>

### Kitap fiyatÄ±nÄ± bulmak â€” element iÃ§inden sayÄ±ya dÃ¶nÃ¼ÅŸtÃ¼rmek

Harika! Åimdi kitap fiyatÄ±nÄ± elde etmeye Ã§alÄ±ÅŸalÄ±m. Browserâ€™daki element incelemesinden fiyatÄ±n `<p class="price_color"></p>` iÃ§inde olduÄŸunu gÃ¶rebilirsin. Bu deÄŸeri `book_price` deÄŸiÅŸkenine ata ve dikkat et, `float` tipinde olmalÄ±!

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le (sadece <strong>kendin denedikten sonra</strong>!)
</summary>

KitaplarÄ± seÃ§erken yaptÄ±ÄŸÄ±mÄ±z gibi burada da CSS class ile seÃ§me yaklaÅŸÄ±mÄ±nÄ± kullanacaÄŸÄ±z ve [`.string`](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#string) methodâ€™undan faydalanacaÄŸÄ±z:

```python
books_html[0].find("p", class_="price_color").string
```

Burada amacÄ±mÄ±z sadece text deÄŸil, **sayÄ±** (Python `float`) elde etmektir. Bunun iÃ§in ilk karakter olan `Â£` iÅŸaretini slice yÃ¶ntemiyle kaldÄ±rmalÄ± ve ardÄ±ndan `float()` methodâ€™una gÃ¶ndermeliyiz:

```python
book_price = float(books_html[0].find("p", class_="price_color").string[1:])
book_price
```

</details>

### Kitap rating bilgisini bulmak

Son olarak kitabÄ±n **rating** bilgisini (kaÃ§ sarÄ± yÄ±ldÄ±zÄ± olduÄŸu) elde etmeliyiz. Browser inspectorâ€™da `<p class="star-rating TEXT"></p>` ÅŸeklinde bir yapÄ± gÃ¶receksin. Buradaki `TEXT` ÅŸu deÄŸerlerden biri olabilir: "One", "Two", "Three", "Four" veya "Five". Bu biraz daha karmaÅŸÄ±k olsa da yapÄ±labilir. Yeni bir cell aÃ§ ve aÅŸaÄŸÄ±daki kodu kopyala/yapÄ±ÅŸtÄ±r:

```python
book_stars_html = books_html[0].find("p", class_="star-rating")
book_stars_html
```

```python
book_stars_html.attrs['class']
```

Pythonâ€™da `in` keywordâ€™Ã¼ bir Ã¶ÄŸenin bir `list` iÃ§inde olup olmadÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in kullanÄ±lÄ±r. Ã–rneÄŸin:

```python
cities = [ 'paris', 'london', 'brussels' ]

if 'berlin' in cities:
    print("Berlin is available")
else:
    print("Sorry, Berlin is not available")
```

:question: `<p />` iÃ§inden gelen class listâ€™ini alÄ±p 1 ile 5 arasÄ±nda rating dÃ¶ndÃ¼ren bir `parse_rating` methodâ€™u tanÄ±mla:

```python
def parse_rating(rating_classes):
    # TODO: Look at `rating_classes` and return the correct rating
    # e.g. of an argument for `rating_classes`: [ 'star-rating', 'Three' ]
    # "One" => 1
    # "Two" => 2
    # "Three" => 3
    # "Four" => 4
    # "Five" => 5
    return 0
```

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le (sadece <strong>kendin denedikten sonra</strong>!)
</summary>

```python
def parse_rating(rating_classes):
    """
    Parses the rating classes and returns the rating

    Parameters
    ----------
    rating_classes : str
        The rating classes of the book: these are the classes of the stars element in the HTML

    Examples
    --------
    >>> rating_classes = ['star-rating', 'Three']
    >>> parse_rating(rating_classes)
    3
    """
    # Define the ratings: mapping from English to numerical
    ratings = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}
    # For each of the 5 possible ratings, check if it's in the rating classes
    for rating in ratings:
        if rating in rating_classes:
            return ratings[rating] # Found the rating, return the numerical value
```

</details>

Bu methodâ€™u implement ettikten sonra kitabÄ±n rating deÄŸerini okuyabilirsin! Yeni bir cell aÃ§ ve aÅŸaÄŸÄ±daki kodu yapÄ±ÅŸtÄ±r:

```python
book_rating = parse_rating(books_html[0].find("p", class_="star-rating").attrs['class'])
```

## Parsing *all* books

Bir kez daha, yeni bir **Markdown cell** ekleyip ÅŸunu yazmanÄ±n zamanÄ±:

```markdown
## Parsing _all_ books
```

Åimdiye kadar yalnÄ±zca **ilk** kitap iÃ§in parsing kodu yazdÄ±k. ArtÄ±k bu kodu `books_html` deÄŸiÅŸkenindeki tÃ¼m kitaplar Ã¼zerinde Ã§alÄ±ÅŸacak bir `for` loop iÃ§ine yerleÅŸtirmemiz gerekiyor!

Toplanan kitap bilgilerini bir **Python `dict`** iÃ§inde saklayacaÄŸÄ±z. Bu dictionary Ã¼Ã§ keyâ€™e sahip olacak: `Title`, `Price` ve `Rating`. Bu keyâ€™lerin values kÄ±smÄ± ise tÃ¼m kitaplarÄ±n valuesâ€™larÄ±nÄ± iÃ§eren `list`â€™ler olacaktÄ±r:

* `Title` => `["A light in the attic", "Tipping the Velvet", ...]`
* `Price` => `[51.77, 53.74, ...]`
* `Rating` => `[3, 1, ...]`

Bu yapÄ±yÄ± seÃ§iyoruz Ã§Ã¼nkÃ¼ bu format Pandasâ€™a doÄŸrudan veri aktarmamÄ±zÄ± ve kolaylÄ±kla bir Dataframe oluÅŸturmamÄ±zÄ± saÄŸlar.

Yeni bir cell ekleyerek dictionaryâ€™i baÅŸlat:

```python
books_dict = { 'Title': [], 'Price': [], 'Rating': [] }
```

:question: `books_html` Ã¼zerinde iterate ederek `books_dict` iÃ§ini dolduracak bir loop yaz ve yukarÄ±daki tÃ¼m kodu tekrar kullan.

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le (sadece <strong>kendin denedikten sonra</strong>!)
</summary>

```python
for book in books_html:
    title = book.find("h3").find("a").attrs["title"]
    price = float(book.find("p", class_="price_color").text[1:])
    rating = parse_rating(book.find("p", class_="star-rating").attrs['class'])
    books_dict["Title"].append(title)
    books_dict["Price"].append(price)
    books_dict["Rating"].append(rating)
```

</details>

SonuÃ§larÄ± kontrol etmek iÃ§in aÅŸaÄŸÄ±daki cellâ€™leri Ã§alÄ±ÅŸtÄ±r:

```python
books_dict
```

```python
len(books_dict)          # 3 key:value Ã§ifti olmalÄ±
```

```python
len(books_dict["Title"]) # Her listede 20 kitap olmalÄ±
```

## Pandas'a Veri YÃ¼kleme

Yeni bir section! SÃ¼reci dokÃ¼mante etmek iÃ§in yeni bir **Markdown cell** oluÅŸturmayÄ± unutma.

`books_dict` iyi gÃ¶rÃ¼nÃ¼yor, ÅŸimdi bu veriyi Pandasâ€™a yÃ¼kleyelim:

```python
books_df = pd.DataFrame(books_dict)
books_df
```

Harika gÃ¶rÃ¼nÃ¼yor! KÃ¼Ã§Ã¼k bir plot oluÅŸturarak kutlayalÄ±m. Bu grafik her **Rating** deÄŸeri iÃ§in kaÃ§ kitap olduÄŸunu gÃ¶sterecek:

```python
books_df.groupby("Rating").count()["Title"].plot(kind="bar")
```

### Kodunu test et!

Test etmek iÃ§in aÅŸaÄŸÄ±daki cellâ€™i ekle ve Ã§alÄ±ÅŸtÄ±r:

```python
from nbresult import ChallengeResult

result = ChallengeResult('books',
    columns=books_df.columns,
    title=str(books_df.loc[0,'Title']),
    price=books_df.loc[0,'Price'],
    rating=books_df.loc[0,'Rating']
)
result.write()
print(result.check())
```

ArdÄ±ndan kodunu `commit` ve `push` edebilirsin ğŸš€

OldukÃ§a fazla kitabÄ±n dÃ¼ÅŸÃ¼k rating (1) aldÄ±ÄŸÄ±nÄ± gÃ¶rebilirsin. Belki de bu sadece ilk sayfadaki kitaplara Ã¶zgÃ¼dÃ¼r? Peki ya **diÄŸer** sayfalar?

## TÃ¼m katalog sayfalarÄ±nÄ± gezmek

Yeni bir bÃ¶lÃ¼m! **Markdown cell** eklemeyi unutma.

[books.toscrape.com](http://books.toscrape.com/) sitesinde en alta in ve "Next" butonuna tÄ±kla. Tekrar tÄ±kla. FarklÄ± sayfalar iÃ§in URL patternâ€™Ä±nÄ± gÃ¶rebiliyor musun?

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le
</summary>

```python
page = 1
url = f"http://books.toscrape.com/catalogue/page-{page}.html"
url
```

</details>

Bir `for` loop daha gerekiyor! Bu loop 1â€™den 50â€™ye kadar tÃ¼m sayfalarÄ± gezerek scraping yapacak. Åimdilik test iÃ§in 1â€“3 arasÄ± sayfalarÄ± alalÄ±m:

```python
MAX_PAGE = 3
for page in range(1, MAX_PAGE + 1):
    url = f"http://books.toscrape.com/catalogue/page-{page}.html"
    print(url)
```

Loop Ã§alÄ±ÅŸÄ±yor gibi gÃ¶rÃ¼nÃ¼yor! ArtÄ±k `print` yerine gerÃ§ek scraping kodunu yazabiliriz.

<details><summary markdown='span'>Ã‡Ã¶zÃ¼mÃ¼ gÃ¶rÃ¼ntÃ¼le
</summary>

```python
all_books_dict = { 'Title': [], 'Price': [], 'Rating': [] }

MAX_PAGE = 50
for page in range(1, MAX_PAGE + 1):
    print(f"Parsing page {page}...")
    url = f"http://books.toscrape.com/catalogue/page-{page}.html"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")

    for book in soup.find_all("article", class_="product_pod"):
        title = book.find("h3").find("a").attrs["title"]
        price = float(book.find("p", class_="price_color").text[1:])
        rating = parse_rating(book.find("p", class_="star-rating").attrs["class"])
        all_books_dict["Title"].append(title)
        all_books_dict["Price"].append(price)
        all_books_dict["Rating"].append(rating)

print("Done!")
```

</details>

Åimdi gerÃ§ekten `MAX_PAGE * 20` kitap parse edildi mi kontrol et:

```python
len(all_books_dict["Title"])
```

Åimdi `all_books_dict` verisini Pandas DataFrameâ€™e dÃ¶nÃ¼ÅŸtÃ¼relim:

```python
all_books_df = pd.DataFrame.from_dict(all_books_dict)
all_books_df.tail()
```

KitaplarÄ±n fiyat daÄŸÄ±lÄ±mÄ±nÄ± inceleyelim:

```python
all_books_df["Price"].hist()
```

Ve rating daÄŸÄ±lÄ±mÄ±nÄ± gÃ¶relim:

```python
all_books_df.groupby("Rating").count()["Title"].plot(kind="bar")
```

## Veriyi daha sonra kullanmak iÃ§in kaydetmek

Åu anda tÃ¼m scraped data Notebookâ€™un memoryâ€™si iÃ§inde yer almakta ve kernel yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda kaybolacaktÄ±r. Bu nedenle, scraping iÅŸlemi sonrasÄ± veriyi bir dosyaya kaydetmek iyi bir pratiktir.

Pandasâ€™Ä±n sunduÄŸu [writers](https://pandas.pydata.org/docs/user_guide/io.html) yardÄ±mÄ±yla DataFrameâ€™i diske yazabiliriz:

```python
all_books_df.to_csv("books.csv")
```

Excel formatÄ±nda da kaydedebilirsin:

```bash
pip install XlsxWriter
```

```python
all_books_df.to_excel('books.xlsx', sheet_name='Books')
```

Ä°yi bir pratik, **Data Pipeline** oluÅŸturarak bir sÃ¼recin veriyi scrape edip CSVâ€™ye yazmasÄ±, diÄŸer bir sÃ¼recin ise bu veriyi CSVâ€™den okuyarak analiz etmesidir.

ğŸ’¡ Kodunu GitHubâ€™a **push** etmeyi unutma

## Refactoring hakkÄ±nda

Birbirinden farklÄ± iÅŸler yapan oldukÃ§a fazla kod yazdÄ±k. Bu kodu daha okunabilir ve tekrar kullanÄ±labilir hale getirmek iÃ§in daha kÃ¼Ã§Ã¼k functionâ€™lara bÃ¶lmek iyi bir pratiktir:

* `fetch_page` â†’ Tek bir sayfayÄ± alÄ±r ve `soup` oluÅŸturur
* `add_books_to_dict` â†’ Tek bir sayfadaki kitap bilgilerini dictionaryâ€™e ekler
* `create_books_df` â†’ Belirli sayÄ±da sayfayÄ± gezer, Ã¶nceki iki functionâ€™Ä± kullanarak DataFrame oluÅŸturur

ZamanÄ±n varsa bir sonraki aÅŸamaya geÃ§meden Ã¶nce kodunu bu ÅŸekilde refactor etmeyi dene.

EÄŸer vaktin yoksa, Ã§Ã¶zÃ¼m yayÄ±nlandÄ±ÄŸÄ±nda gÃ¶z atarak kodun okunabilirliÄŸinin nasÄ±l iyileÅŸtiÄŸini inceleyebilirsin!
